prompt_settings:
  template: "Translate the following text from English into Spanish.\nEnglish: {SLsegment} \nSpanish:"
  # Set to the key name (e.g., "translation") to extract from a JSON response.
  # Set to "None" to disable JSON parsing.
  json_key: None #"translation"
  # Regex still works as a second filter after JSON extraction
  regex_pattern: None   


# --- MODEL CONFIGURATION ---
model_settings:
  # The Hugging Face model ID or local path to the model directory
  name: "BSC-LT/salamandraTA-2b-instruct"
  # Computing device: "cuda" for NVIDIA GPU (much faster) or "cpu"
  device: "cuda"
  # Must be true for models that use custom code on Hugging Face (like Salamandra)
  trust_remote_code: true

# --- LLM GENERATION PARAMETERS ---
generation_params:
  # Maximum number of new tokens to generate (excluding the prompt)
  max_new_tokens: 128
  # Temperature: 0.0 for deterministic/greedy search (best for translation).
  # Values > 0.0 increase creativity/randomness.
  temperature: 0.0
  # Penalizes repeated tokens to prevent the model from getting stuck in loops.
  # 1.0 means no penalty; values like 1.1 or 1.2 are common.
  repetition_penalty: 1.2
  # Filters the top-K most likely next words. Low values (e.g., 40) limit nonsense.
  top_k: 40
  # Nucleus sampling: only considers tokens with a cumulative probability of P.
  # Usually set between 0.8 and 0.95.
  top_p: 0.9
  # If set > 0, the model will not repeat any sequence of N words.
  # Excellent for preventing repetitive "echoes" in the output.
  no_repeat_ngram_size: 3
  # --- STOPPING CONTROLS ---
  # List of strings that will force the model to stop generating immediately.
  # "\\n" is the escaped newline character.
  stop_sequences: ["\\n", "###", "User:"]
  # Use the model's official End-Of-Sentence token to stop generation.
  use_eos_token: true


